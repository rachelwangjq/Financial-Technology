---
title: "FinTech_Assignment1"
author: "Jiaqiu Wang"
date: "February 13, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### FNCE 385/885 Assignment 1: Credit Modeling

## Initialization & Data Preparation

```{r}
rm(list = ls()) # Clear the memory
library("pROC") # The package needed for plotting ROC curves

## Load data
# Load the data from the data file. The first row is variable names.
# To avoid trouble we do not use "string as factor" option
Data_set <- read.csv("File1_IS_data.csv",header = TRUE) 

#check the data type of each column in the data set
str(Data_set)

# Transform "defaulted" into a binomial response, which gives 1 if defaulted or zero otherwise.
Data_set$default <- Data_set$default == "Defaulted" 

```

## Q1. Basic model estimate

a. Before estimating the model, I would expect to see the coefficient for fico score is negative, since people with a higher fico score tends to have less probability of default.

```{r}
#b. First logistic regression
model1 <- glm(default ~ fico, family = "binomial", data = Data_set)
# family = "binominal" tells R to run a logistic regression.
summary(model1)
```

(b) The estimates for the intercept and the coefficient for fico from the logistic model are 7.68 and -0.013 respectively. The result suggest that fico score has significant explanatory power with a near-zero p-value.

## Q2. Model evaluation

```{r}
#a. Get the fitted default probability
Data_set$predicted_p <- predict(model1, type = "response")
#'type = "response"' tells R to give estimated probability of default directly

#b. Now we can draw a ROC curve.
ROC1 <- roc(default ~ predicted_p, data = Data_set) # Calculate the ROC curve
plot(ROC1) # Plot the ROC curve

```

```{r}
#c. calculate the area under the ROC curve
# With package pROC you can use the function auc()
auc(default ~ predicted_p, data = Data_set)
```

The area under the ROC curve is 0.5981 which is greater than 0.5, this is consistent with the results in part1(b), since fico score has significant explanatory power. 

```{r}
#d. Calculating the percentage of false positives and true positive with cut-off 0.1: 
#For any one who has estimated probability of default being 0.1, we label them as 'default'.
Data_set$Predicted_default <- Data_set$predicted_p > 0.1
Num_predicted_pos <- sum(Data_set$Predicted_default == TRUE)
Num_correct_pos_pred <- sum(Data_set$Predicted_default == TRUE & Data_set$default == TRUE)
true_positive <- Num_correct_pos_pred/Num_predicted_pos
false_positive <- 1-true_positive
#display the result for percentage of `correct' positive
cat("true positive rate:",true_positive)
cat("false positive rate:", false_positive)

```

The proportion of consumers you mistakenly reject (false positives) is 82.6%, the proportion of consumer you correctly reject (true positives) is 17.4%.

## Q3.An out-of-sample analysis

```{r}
#a. Create a subsample data set with first 9000 samples. The remaining data goes to test data
Data_set_training <- Data_set[1:9000,]
Data_set_test <- Data_set[9001:nrow(Data_set),]
names(Data_set_test)
model2 <- glm(default ~ fico, family = "binomial", data = Data_set_training)
summary(model2)
```

The estimates for the intercept and the coefficient for fico using only the training dataset are 7.16 and -0.013 respectively. The estimated model is close to the model we get for Q1.

```{r}
#b. Compare the performance
Data_set_test$predicted_p = predict(model2, newdata = Data_set_test, type = "response")
# Calculate the ROC curve
ROC1 <- roc(default ~ predicted_p, data = Data_set) 
ROC2 <- roc(default ~ predicted_p, data = Data_set_test)

plot(ROC1, col = "red") # Plot the ROC curve, 'col = "red"' sets the color of the 
# curve to be red.
plot(ROC2, add = TRUE, col = "blue") # The argument 'add = TURE' makes sure that the curve is added
```

c. The area below the new ROC line get larger compared with what we get earlier. Here since we just took the first 9000 records as training dataset instead of randomly sampling the data, the dataset may have some bias itself and the model just happen to fit better on the remaining 1000 data which we used as teating data.

d. As a manager, with the opportunity to conduct a multi-variate logistic regression analysis, I'm not going to use all variables available, since some of the variables are correlated themselves and some of the variables don't have much explainatory power. If adding those variables to the model the coefficients for the variables with actual explainatory power may be biased.

```{r}
#e. A more complicated model
model3 <- glm(default ~fico + loan_amnt + int_rate + verification_status, family = "binomial", data = Data_set)
summary(model3)
Data_set$predicted_p_new <- predict(model3, type = "response")

# Now we can draw a new ROC curve.
ROC3 <- roc(default ~ predicted_p_new, data = Data_set) # Calculate the ROC curve
plot(ROC1, col = "red") # Plot the ROC curve
plot(ROC3, add = TRUE, col = "green")
```

I added loan amount, interest rate and the employment varification status to the logistic regression model, the coefficients are only siginificant for fico socre and interest rate. Drawing the ROC curve, we can see the model performance get improved with the new variable since the area under ROC curve is larger. When adding interest rate to the model, the coeffcient estimate for fico score decrease to almost zero, which means interest rate includes almost all the infomation we can get from the fico score but also some other information. However, when reporting the final model, I will not keep any of the new variables, since the interest rate information will not be available when assessing the loan in real life.
